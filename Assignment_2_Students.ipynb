{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwQSBXOl4d1I1QTM41yHzZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HumanAndMachineHearing/Practical_2023/blob/main/Assignment_2_Students.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Programming Assignment 2. Audio feature extraction for sound classification**\n",
        "\n",
        "# 1. Introduction\n",
        "<p align = \"justify\"> In the previous session, you explored the ESC-50 dataset and analyzed and visualized sound waves, spectra, and spectrograms of sound clips in the ESC-50 dataset. In this session, you will dive deeper into the topic of audio feature extraction.\n",
        "\n",
        "**Practical Report**\n",
        "<br>\n",
        "For this session, you are expected to add the output of and answers to the exercises as defined in the notebook to the Practical Report. A link to the templates for the practical reports can be found in the Readme file."
      ],
      "metadata": {
        "id": "V3Hk7p6DyhFY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#Preparation\n",
        "\n",
        "Before embarking on the exercises, import the libraries that are essential for this excerise."
      ],
      "metadata": {
        "id": "fa2wGWB7_pnO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPTkwztgyeL8"
      },
      "outputs": [],
      "source": [
        "# Machine learning framework\n",
        "import torch\n",
        "\n",
        "# Library for audio and signal processing with PyTorch\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T # For common audio processings and feature extractions. Implements features as objects\n",
        "import torchaudio.functional as F # Implements features as standalone functions\n",
        "\n",
        "# For manipulating directory paths\n",
        "import os\n",
        "\n",
        "# For working with datasets\n",
        "import pandas as pd\n",
        "\n",
        "# Plotting library\n",
        "import matplotlib.pyplot as plt\n",
        "# To embed plots within the notebook\n",
        "%matplotlib inline\n",
        "\n",
        "# Scientific and vector computation for Python\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sound selection\n",
        "\n",
        "Use the same selection of example sounds as in Assignment 1."
      ],
      "metadata": {
        "id": "afYDC8BtKpAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add code to load example sounds with Torchaudio"
      ],
      "metadata": {
        "id": "HrjvF4NLWeoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Mel spectrogram\n",
        "\n",
        "<p align = \"justify\"> In Session 1, you calculated time-frequency representations of sound clips as power spectrograms. The frequency axis of these spectrograms was linear from 1 Hz to 22 kHz. In this exercise, you will use the Mel filterbank to calculate an alternative spectrogram representation that resembles human hearing properties.\n",
        "\n",
        "## Visualizing the Mel filter bank\n",
        "**Exercise 2.1:**\n",
        "<p align = \"justify\"> (A) Try different numbers of Mel filters and plot the resulting filter banks.\n",
        "<p align = \"justify\"> (B) Describe how the Mel filterbank aims to mimic human hearing properties and why it is beneficial for a Machine Hearing model to receive input that resembles these human hearing properties.\n",
        "\n"
      ],
      "metadata": {
        "id": "exQdeAzyAcTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify parameters for mel filterbank\n",
        "mgram_n_fft = 1024\n",
        "mgram_n_mels = 64\n",
        "mgram_f_min = 1 # set minimum frequency\n",
        "mgram_f_max = sample_rate / 2 # Nyquist frequency; maximum frequency"
      ],
      "metadata": {
        "id": "Y1z9n5HwC9Ne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define mel filterbank\n",
        "mel_filters = F.melscale_fbanks(\n",
        "    int(mgram_n_fft // 2 + 1), #Audio f\n",
        "    n_mels= mgram_n_mels,\n",
        "    f_min= mgram_f_min,\n",
        "    f_max= mgram_f_max,\n",
        "    sample_rate=sample_rate,\n",
        "    norm = None, # choose 'Slaney' for area normalization\n",
        "    mel_scale = 'htk',\n",
        ")"
      ],
      "metadata": {
        "id": "dDtyYFXlSfFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To calculate the center frequencies of the mel filters, first calculate the center mel of\n",
        "# the minimum and maximum\n",
        "\n",
        "# extract center frequencies of mel filters\n",
        "mel_min = 2595*np.log10(1+(mgram_f_min/700))\n",
        "mel_max = 2595*np.log10(1+(mgram_f_max/700))\n",
        "bin_width = (mel_max-mel_min)/(mgram_n_mels+1)\n",
        "mel_axis = np.linspace(mel_min+(bin_width/2),mel_max-(bin_width/2),mgram_n_mels) # shift the bins so they do not exceed the edges\n",
        "mel_axis_infreq = (np.round(700*(np.power(10,(mel_axis/2595))-1))).astype(int) # calculate frequency axis\n"
      ],
      "metadata": {
        "id": "Gk3UeqLxwW-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # calculate frequency axis\n",
        "mspectrogram_freqs = (np.round(np.arange(0,np.shape(mel_filters)[0])*sample_rate/mgram_n_fft)).astype(int)\n",
        "\n",
        "# Visualize mel filterbank as 2D image\n",
        "fig = plt.figure()\n",
        "plt.title(\"Mel filter bank\")\n",
        "plt.imshow(mel_filters, aspect=\"auto\", origin = 'lower', cmap = 'jet', vmin = 0, vmax = 1)\n",
        "plt.colorbar(label = 'amplitude')\n",
        "plt.ylabel(\"Frequency (Hertz)\");\n",
        "plt.xlabel(\"Mel bin (number)\");\n",
        "ytickdefinition = [0, 100, 200, 300, 400, 500]\n",
        "plt.yticks(ytickdefinition,mspectrogram_freqs[ytickdefinition]);\n",
        "xtickdefinition = [0,10,20,30,40,50,60]\n",
        "#plt.xticks(xtickdefinition,mel_axis_infreq[xtickdefinition]); # use this to label axis with Mel center frequencies rather than filter number\n",
        "\n",
        "# Visualize mel filterbank as line plot\n",
        "fig = plt.figure()\n",
        "plt.plot(mel_filters);\n",
        "plt.title('Mel filters visualized')\n",
        "plt.ylabel('Amplitude');\n",
        "plt.xlabel('Frequency (Herz)')\n",
        "xtickdefinition = [0, 100, 200, 300, 400, 500]\n",
        "plt.xticks(xtickdefinition,mspectrogram_freqs[xtickdefinition]);"
      ],
      "metadata": {
        "id": "lz3XFoqGTjoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate and visualize the Log-Mel spectrogram\n",
        "\n",
        "**Exercise 2.2:**\n",
        "<p align = 'justify'> Calculate and plot Log-Mel spectrograms of the sounds that you selected. Compare the Log-Mel spectrogram plots to the spectrogram plots of Assignment 1 to describe what the effect is of using a Mel filterbank on the time-frequency representation of a sound."
      ],
      "metadata": {
        "id": "l_vUVaU2del8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify parameters for mel spectrogram\n",
        "mgram_win_length = 512 # window size; default = n_fft; determines the resolution of the time axis\n",
        "mgram_hop_length = 256 # length of hop between STFT windows; default = win_legth / 2; # determines the resolution of the time axis"
      ],
      "metadata": {
        "id": "G_NJB_mvds5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define  transform to mel spectrogram, either magnitude or power\n",
        "mel_spectrogram = T.MelSpectrogram(\n",
        "    sample_rate = sample_rate,\n",
        "    n_fft = mgram_n_fft,\n",
        "    win_length = mgram_win_length,\n",
        "    hop_length = mgram_hop_length,\n",
        "    center = True,\n",
        "    pad_mode = \"reflect\",\n",
        "    power = 2.0,\n",
        "    norm = None,\n",
        "    n_mels = mgram_n_mels,\n",
        "    mel_scale = \"htk\",\n",
        ")"
      ],
      "metadata": {
        "id": "jT2iOIDFdonF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform transform to Mel spectrogram\n",
        "melspec = mel_spectrogram(waveform)\n",
        "\n",
        "# Define time axis\n",
        "mspectrogram_time_axis = np.round(np.arange(0,np.shape(melspec)[2])*((np.shape(waveform)[1]/np.shape(melspec)[2])/sample_rate),2)\n",
        "\n",
        "# Convert units of power spectrogram to decibel\n",
        "offset_val = 1e-10 # add small offset to avoid taking logarithm of zero\n",
        "melspec_db = 10 * np.log10(melspec+offset_val)"
      ],
      "metadata": {
        "id": "BM8wNtrYeNdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from re import X\n",
        "# Convert to dB to create log-mel spectrogram\n",
        "fig = plt.figure(figsize = (10,6))\n",
        "plt.title('Log-Mel Spectrogram NAME SOUND')\n",
        "plt.ylabel('Center Frequency (Hz)')\n",
        "plt.xlabel('Time (seconds)')\n",
        "plt.imshow(np.squeeze(melspec_db), origin = 'lower', cmap = 'jet', vmin = -30, vmax = 30, aspect = 3) # aspect sets the aspect ratio of the pixels\n",
        "plt.colorbar(shrink = 0.3,label = 'decibel'); # add colorbar of same size as original plot\n",
        "xtickdefinition = [0,100,200,300,400,500,600,700,800,861];\n",
        "plt.xticks(xtickdefinition,mspectrogram_time_axis[xtickdefinition]);\n",
        "ytickdefinition = [0,10,20,30,40,50,60];\n",
        "plt.yticks(ytickdefinition,mel_axis_infreq[ytickdefinition]);\n"
      ],
      "metadata": {
        "id": "vsF8O5vvpYo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Mel frequency cepstral coefficients\n",
        "The final audio feature that you will investigate in this practical is the Mel Frequency Cepstral Coefficient (MFCC).\n",
        "\n",
        "**Exercise 2.3:**\n",
        "Calculate and plot MFCCs of the sounds that you selected and add them to your report."
      ],
      "metadata": {
        "id": "AY6qjwJXI3CU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify parameters MFCC\n",
        "mfcc_n_fft = 1024 # include descriptions of these parameters.\n",
        "mfcc_win_length = 512\n",
        "mfcc_hop_length = 256\n",
        "mfcc_n_mels = 64\n",
        "mfcc_n_mfcc = 64"
      ],
      "metadata": {
        "id": "iZcHvqtVI8hC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define MFCC transform\n",
        "\n",
        "mfcc_transform = T.MFCC(\n",
        "    sample_rate = sample_rate,\n",
        "    n_mfcc = mfcc_n_mfcc,\n",
        "    melkwargs ={\n",
        "        \"n_fft\": mfcc_n_fft,\n",
        "        \"n_mels\": mfcc_n_mels,\n",
        "        \"hop_length\": mfcc_hop_length,\n",
        "        \"mel_scale\": \"htk\",\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "id": "-k3hAzvl-RE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform MFCC transform\n",
        "mfcc_ex = mfcc_transform(waveform)"
      ],
      "metadata": {
        "id": "YcxJ2F56-jgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot MFCC\n",
        "fig = plt.figure(figsize=(10,6))\n",
        "plt.title('MFCC ')\n",
        "plt.ylabel('Mel frequency bin')\n",
        "plt.xlabel('Time (seconds)')\n",
        "plt.imshow(np.squeeze(mfcc_ex[0]), origin = 'lower', cmap = 'jet', vmin = -30, vmax = 30, aspect = 3)\n",
        "plt.colorbar(shrink = 0.3, label = 'Coefficient');\n",
        "xtickdefinition = [0,100,200,300,400,500,600,700,800,861];\n",
        "plt.xticks(xtickdefinition,mspectrogram_time_axis[xtickdefinition]);\n"
      ],
      "metadata": {
        "id": "tWK_8st_-qKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Preparing audio features for Resnet-18\n",
        "\n",
        "<p align=\"justify\"> The Resnet model was originally developed for image classification. This means that the implementation we use here can take either one-channel input (corresponding to grayscale images in the original implementation) or three-channel input (corresponding to RGB images in the original implementation). Furthermore, the network expects square input.\n",
        "\n",
        "<p align=\"justify\"> During this practical, you will train the ResNet-18 model on one-channel audio input (corresponding to one audio feature) and on three-channel input (corresponding to a combination of three audio features) to investigate the relevance of various audio features on sound classification performance.\n",
        "\n",
        "<p align=\"justify\">Given the computational load of training the model, it is recommended to generate features of dimensions 128 x 128. For the time dimension, cut sound clips to 3 seconds duration instead of 5 seconds.\n",
        "\n",
        "**Exercise 2.4:**\n",
        "<br>\n",
        "<p align=\"justify\"> Based on your analysis of audio features in this practical, select which audio feature you will use as one-channel input and which combination of features you will use as three-channel input. Motivate your choice and use illustrations of the audio features to support your motivation. Also describe the parameters that you used for generating the audio features (e.g., 128 x 128).  \n",
        "\n"
      ],
      "metadata": {
        "id": "xAfEITqfSL3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add your code here"
      ],
      "metadata": {
        "id": "dDRFuHhwDJ0v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}